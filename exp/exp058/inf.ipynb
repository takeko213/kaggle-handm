{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import itertools\n",
    "import pickle\n",
    "import pathlib\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append(os.getenv('UTILS_PATH'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import line_notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import types\n",
    "\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        # module imports\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield name, val\n",
    "\n",
    "            # functions / callables\n",
    "        if hasattr(val, '__call__'):\n",
    "            yield name, val\n",
    "\n",
    "\n",
    "def noglobal(f):\n",
    "    '''\n",
    "    ref: https://gist.github.com/raven38/4e4c3c7a179283c441f575d6e375510c\n",
    "    '''\n",
    "    return types.FunctionType(f.__code__,\n",
    "                              dict(imports()),\n",
    "                              f.__name__,\n",
    "                              f.__defaults__,\n",
    "                              f.__closure__\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "RUN_INF = True # 推論処理を行うか\n",
    "BATCH_SIZE = int(1e5)\n",
    "N_ITER = 20 # 学習データのローリング数\n",
    "RUN_US = True # アンダーサンプリング実施有無\n",
    "N_SEED = 10 # seed avgの回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = {}\n",
    "Ns['cf_a'] = 12\n",
    "Ns['ctf_a'] = 12\n",
    "Ns['atfd_a'] = 12\n",
    "Ns['atfp_a'] = 12\n",
    "Ns['pa_a'] = 12\n",
    "\n",
    "Ns['cf_w'] = 12\n",
    "Ns['ctf_w'] = 12\n",
    "Ns['atfd_w'] = 12\n",
    "Ns['atfp_w'] = 12\n",
    "Ns['pa_w'] = 12\n",
    "\n",
    "Ns['cf_m'] = 12\n",
    "Ns['ctf_m'] = 12\n",
    "Ns['atfd_m'] = 12\n",
    "Ns['atfp_m'] = 12\n",
    "Ns['pa_m'] = 12\n",
    "\n",
    "Ns['cf_y'] = 12\n",
    "Ns['ctf_y'] = 12\n",
    "Ns['atfd_y'] = 12\n",
    "Ns['atfp_y'] = 12\n",
    "Ns['pa_y'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディレクトリ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "#exp_name = os.path.dirname(__file__).split('/')[-1]\n",
    "exp_name = 'exp058'\n",
    "os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(INPUT_DIR + 'articles.csv', dtype='object')\n",
    "customers = pd.read_csv(INPUT_DIR + 'customers.csv')\n",
    "transactions = pd.read_csv(INPUT_DIR + 'transactions_train.csv', dtype={'article_id':'str'}, parse_dates=['t_dat'])\n",
    "sample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week_sales_pred = pd.read_csv(OUTPUT_DIR + '1st_week_sales_pred_v004/result.csv', dtype={'article_id':'str'},  parse_dates=['1st_week_sales_dat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CUSTOMER = customers['customer_id'].unique().tolist()\n",
    "ALL_ARTICLE = articles['article_id'].unique().tolist()\n",
    "\n",
    "customer_ids = dict(list(enumerate(ALL_CUSTOMER)))\n",
    "article_ids = dict(list(enumerate(ALL_ARTICLE)))\n",
    "\n",
    "customer_map = {u: uidx for uidx, u in customer_ids.items()}\n",
    "article_map = {i: iidx for iidx, i in article_ids.items()}\n",
    "\n",
    "articles['article_id'] = articles['article_id'].map(article_map)\n",
    "customers['customer_id'] = customers['customer_id'].map(customer_map)\n",
    "transactions['article_id'] = transactions['article_id'].map(article_map)\n",
    "transactions['customer_id'] = transactions['customer_id'].map(customer_map)\n",
    "sample['customer_id'] = sample['customer_id'].map(customer_map)\n",
    "first_week_sales_pred['article_id'] = first_week_sales_pred['article_id'].map(article_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名寄せ\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].str.replace('None','NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['age10'] = str((customers['age'] // 10) * 10)\n",
    "customers.loc[customers['age'].isnull(), 'age10'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoding\n",
    "le_cols = ['product_type_name', 'product_group_name', 'graphical_appearance_name',\n",
    "            'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    "            'index_name', 'section_name', 'garment_group_name']\n",
    "for c in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    articles[c] = le.fit_transform(articles[c].fillna(''))\n",
    "\n",
    "\n",
    "le_cols = ['club_member_status', 'fashion_news_frequency', 'postal_code', 'age10']\n",
    "for c in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    customers[c] = le.fit_transform(customers[c].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_INDEX_GROUP_NAME = articles['index_group_name'].unique().tolist()\n",
    "index_group_name_ids = dict(list(enumerate(ALL_INDEX_GROUP_NAME)))\n",
    "index_group_name_map = {u: uidx for uidx, u in index_group_name_ids.items()}\n",
    "articles['index_group_name'] = articles['index_group_name'].map(index_group_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['customer_type'] = customers['FN'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['Active'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['club_member_status'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['fashion_news_frequency'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['age10'].fillna(0).astype(int).astype(str)\n",
    "\n",
    "le = LabelEncoder()\n",
    "customers['customer_type'] = le.fit_transform(customers['customer_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactionに紐づけ\n",
    "transactions = transactions.merge(customers, on='customer_id', how='left')\n",
    "transactions = transactions.merge(articles, on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text特徴量\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "detail_desc_n_dim = 15\n",
    "text_col = 'detail_desc'\n",
    "articles[text_col] = articles[text_col].str.lower()\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "text_tfidf = tfidf_vec.fit_transform(articles[text_col].fillna('').values.tolist())\n",
    "svd = TruncatedSVD(n_components=detail_desc_n_dim, algorithm='arpack',random_state=42)\n",
    "text_svd = svd.fit_transform(text_tfidf)\n",
    "text_svd_df = pd.DataFrame(text_svd, columns=[f'{text_col}_svd_{i}' for i in range(detail_desc_n_dim)])\n",
    "text_svd_df = pd.concat([articles[['article_id']], text_svd_df], axis=1)\n",
    "\n",
    "prod_name_n_dim = 15\n",
    "text_col = 'prod_name'\n",
    "articles[text_col] = articles[text_col].str.lower()\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "text_tfidf = tfidf_vec.fit_transform(articles[text_col].fillna('').values.tolist())\n",
    "svd = TruncatedSVD(n_components=prod_name_n_dim, algorithm='arpack',random_state=42)\n",
    "\n",
    "text_svd = svd.fit_transform(text_tfidf)\n",
    "text_svd_df_tmp = pd.DataFrame(text_svd, columns=[f'{text_col}_svd_{i}' for i in range(prod_name_n_dim)])\n",
    "text_svd_df = pd.concat([text_svd_df, text_svd_df_tmp], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセット作成（レコメンド→対象データセット作成→特徴量エンジニアリング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def get_customer_frequent(history, n=12, timedelta=None):\n",
    "    \"\"\"顧客ごと商品の購入数をカウントし上位の商品を抽出\n",
    "\n",
    "    Args:\n",
    "        history (dataframe): 集計対象の実績データ\n",
    "        n (int): レコメンド対象とする数\n",
    "        timedelta (dateutil.relativedelta): 指定された場合、実績データの終端からtimedelta分のデータを取得する\n",
    "\n",
    "    Returns:\n",
    "        dataframe: 抽出結果\n",
    "    \"\"\"\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "        \n",
    "    customer_agg = history.groupby(['customer_id', 'article_id'])['t_dat'].count().reset_index()\n",
    "    customer_agg = customer_agg.rename(columns={'t_dat':'cnt'})\n",
    "    customer_agg = customer_agg.sort_values(['customer_id', 'cnt'], ascending=False)\n",
    "    result = customer_agg.groupby('customer_id').head(n)\n",
    "    return result[['customer_id', 'article_id']]\n",
    "\n",
    "@noglobal\n",
    "def get_popular_article(history, n=12, timedelta=None):\n",
    "    \"\"\"全体の購入数をカウントし上位の商品を抽出\n",
    "\n",
    "    Args:\n",
    "        history (dataframe): 集計対象の実績データ\n",
    "        n (int): レコメンド対象とする数\n",
    "        timedelta (dateutil.relativedelta): 指定された場合、実績データの終端からtimedelta分のデータを取得する\n",
    "\n",
    "    Returns:\n",
    "        list: 抽出結果\n",
    "    \"\"\"\n",
    "    # 全体の購入数量\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "\n",
    "    total_agg = history.groupby('article_id')['t_dat'].count().reset_index()\n",
    "    total_agg = total_agg.rename(columns={'t_dat':'cnt'})\n",
    "    total_agg = total_agg.sort_values(['cnt'], ascending=False)\n",
    "    total_agg = total_agg.head(n)\n",
    "    result = list(total_agg['article_id'].values)\n",
    "    return result\n",
    "\n",
    "@noglobal\n",
    "def get_customer_type_frequent(history, n=12, timedelta=None):\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "\n",
    "    result = history[['customer_id', 'customer_type']].drop_duplicates().copy()\n",
    "    agg = history.groupby(['customer_type', 'article_id'])['t_dat'].count().reset_index()\n",
    "    agg = agg.rename(columns={'t_dat':'cnt'})\n",
    "    agg = agg.sort_values(['customer_type', 'cnt'], ascending=False)\n",
    "    agg = agg.groupby('customer_type').head(n)\n",
    "    result = result.merge(agg[['customer_type', 'article_id']], on='customer_type', how='left')\n",
    "    return result[['customer_id', 'article_id']]\n",
    "\n",
    "@noglobal\n",
    "def get_article_type_frequent(history, col, n=12, timedelta=None):\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "\n",
    "    result = history.groupby(['customer_id', col])['t_dat'].count().reset_index()\n",
    "    result = result.rename(columns={'t_dat':'cnt'})\n",
    "    result = result.sort_values(['customer_id', 'cnt'], ascending=False)\n",
    "    result = result.groupby(['customer_id']).head(1)[['customer_id', col]]\n",
    "\n",
    "    agg = history.groupby([col, 'article_id'])['t_dat'].count().reset_index()\n",
    "    agg = agg.rename(columns={'t_dat':'cnt'})\n",
    "    agg = agg.sort_values([col, 'cnt'], ascending=False)\n",
    "    agg = agg.groupby(col).head(n)\n",
    "    result = result.merge(agg[[col, 'article_id']], on=col, how='left')\n",
    "    return result[['customer_id', 'article_id']]\n",
    "\n",
    "@noglobal\n",
    "def get_popular_new_article(first_week_sales_pred, n=12):\n",
    "    \"\"\"新商品の初週売り上げ予測が高い商品を抽出\n",
    "    \"\"\"\n",
    "    first_week_sales_pred = first_week_sales_pred.sort_values(['1st_week_sales_pred'], ascending=False)\n",
    "    first_week_sales_pred = first_week_sales_pred.head(n)\n",
    "    result = list(first_week_sales_pred['article_id'].values)\n",
    "    return result\n",
    "\n",
    "@noglobal\n",
    "def calc_pair(history):\n",
    "    df = history[['article_id', 't_dat', 'customer_id']].copy()\n",
    "    df = cudf.from_pandas(df)\n",
    "    df['t_dat'] = df['t_dat'].factorize()[0].astype('int16')\n",
    "    dt = df.groupby(['customer_id','t_dat'])['article_id'].agg(list).rename('pair').reset_index()\n",
    "    df = df[['customer_id', 't_dat', 'article_id']].merge(dt, on=['customer_id', 't_dat'], how='left')\n",
    "    del dt\n",
    "    gc.collect()\n",
    "\n",
    "    # Explode the rows vs list of articles\n",
    "    df = df[['article_id', 'pair']].explode(column='pair')\n",
    "    gc.collect()\n",
    "        \n",
    "    # Discard duplicates\n",
    "    df = df.loc[df['article_id']!=df['pair']].reset_index(drop=True)\n",
    "    gc.collect()\n",
    "\n",
    "    # Count how many times each pair combination happens\n",
    "    df = df.groupby(['article_id', 'pair']).size().rename('count').reset_index()\n",
    "    gc.collect()\n",
    "        \n",
    "    # Sort by frequency\n",
    "    df = df.sort_values(['article_id' ,'count'], ascending=False).reset_index(drop=True)\n",
    "    gc.collect()\n",
    "\n",
    "    # pick only top1 most frequent pair\n",
    "    df = df.groupby('article_id').nth(0).reset_index()\n",
    "    pair = dict(zip(df['article_id'].to_arrow().to_pylist(), df['pair'].to_arrow().to_pylist()))\n",
    "\n",
    "    return pair\n",
    "\n",
    "@noglobal\n",
    "def get_reccomend(target_customer_id, history, Ns, first_week_sales_pred):\n",
    "    n = 12\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    td = None\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_a'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_a'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_a'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_a'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_a'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "\n",
    "    popular_new_article = get_popular_new_article(first_week_sales_pred, n=48)\n",
    "    popular_new_article = pd.DataFrame(itertools.product(target_customer_id, popular_new_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_new_article)\n",
    "\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    td = relativedelta(weeks=1)\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_w'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_w'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_w'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_w'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_w'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    td = relativedelta(months=1)\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_m'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_m'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_m'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_m'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_m'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    td = relativedelta(years=1)\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_y'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_y'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_y'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_y'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_y'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    result = result[result['customer_id'].isin(target_customer_id)].copy()\n",
    "\n",
    "    purchased_together_pair = calc_pair(history)\n",
    "    add_result = result.copy()\n",
    "    add_result['article_id'] = add_result['article_id'].map(purchased_together_pair)\n",
    "    result = result.append(add_result.dropna().drop_duplicates())\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def add_labels(recom_result, history):\n",
    "    \"\"\"レコメンドしたデータが学習期間で購入されたかどうかのフラグを付与する\n",
    "\n",
    "    Args:\n",
    "        recom_result (_type_): レコメンド結果\n",
    "        train_tran (_type_): 学習期間のトランザクションデータ\n",
    "\n",
    "    Returns:\n",
    "        _type_: 学習期間での購入フラグを付与したレコメンド結果\n",
    "    \"\"\"\n",
    "    history = history[['customer_id', 'article_id']].drop_duplicates()\n",
    "    history['buy'] = 1\n",
    "    recom_result = recom_result.merge(history, on=['customer_id', 'article_id'], how='left')\n",
    "    recom_result['buy'] = recom_result['buy'].fillna(0)\n",
    "    return recom_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def make_article_features(articles):\n",
    "    cols = ['product_type_name', 'product_group_name', 'graphical_appearance_name',\n",
    "            'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    "            'index_name', 'index_group_name', 'section_name', 'garment_group_name']\n",
    "    return articles[['article_id']+cols]\n",
    "\n",
    "@noglobal\n",
    "def make_article_tran_features(history):\n",
    "    df = history.groupby('article_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'], \n",
    "                                            'age':['max', 'min', 'mean', 'std']}).reset_index()\n",
    "    df.columns = ['article_id','article_total_cnt', 'article_total_latest_buy', 'article_total_1st_buy', 'article_price_max', 'article_price_min', 'article_price_mean', 'article_age_max', 'article_age_min', 'article_age_mean', 'article_age_std']\n",
    "    df['article_total_1st_buy'] = (history['t_dat'].max() - df['article_total_1st_buy']).dt.days\n",
    "    df['article_total_latest_buy'] = (history['t_dat'].max() - df['article_total_latest_buy']).dt.days\n",
    "\n",
    "    history_1weekago = history.loc[(history['t_dat'] > history['t_dat'].max() - relativedelta(days=7)) & \n",
    "                                   (history['t_dat'] <= history['t_dat'].max())]\n",
    "\n",
    "    history_1weekago_df = history_1weekago.groupby('article_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'], \n",
    "                                            'age':['max', 'min', 'mean', 'std', 'median']}).reset_index()\n",
    "    history_1weekago_df.columns = ['article_id','article_total_cnt_1weekago', 'article_total_latest_buy_1weekago', 'article_total_1st_buy_1weekago', 'article_price_max_1weekago', 'article_price_min_1weekago', 'article_price_mean_1weekago', 'article_age_max_1weekago', 'article_age_min_1weekago', 'article_age_mean_1weekago', 'article_age_std_1weekago', 'article_age_median_1weekago']\n",
    "    history_1weekago_df['article_total_1st_buy_1weekago'] = (history_1weekago['t_dat'].max() - history_1weekago_df['article_total_1st_buy_1weekago']).dt.days\n",
    "    history_1weekago_df['article_total_latest_buy_1weekago'] = (history_1weekago['t_dat'].max() - history_1weekago_df['article_total_latest_buy_1weekago']).dt.days\n",
    "\n",
    "    df = pd.merge(df,history_1weekago_df,how='left',on='article_id')\n",
    "\n",
    "    del history_1weekago, history_1weekago_df\n",
    "    gc.collect()\n",
    "\n",
    "    history_2weekago = history.loc[(history['t_dat'] > history['t_dat'].max() - relativedelta(days=14)) & \n",
    "                                   (history['t_dat'] <= history['t_dat'].max() - relativedelta(days=7))]\n",
    "\n",
    "    history_2weekago_df = history_2weekago.groupby('article_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'], \n",
    "                                            'age':['max', 'min', 'mean', 'std', 'median']}).reset_index()\n",
    "    history_2weekago_df.columns = ['article_id','article_total_cnt_2weekago', 'article_total_latest_buy_2weekago', 'article_total_1st_buy_2weekago', 'article_price_max_2weekago', 'article_price_min_2weekago', 'article_price_mean_2weekago', 'article_age_max_2weekago', 'article_age_min_2weekago', 'article_age_mean_2weekago', 'article_age_std_2weekago', 'article_age_median_2weekago']\n",
    "    history_2weekago_df['article_total_1st_buy_2weekago'] = (history_2weekago['t_dat'].max() - history_2weekago_df['article_total_1st_buy_2weekago']).dt.days\n",
    "    history_2weekago_df['article_total_latest_buy_2weekago'] = (history_2weekago['t_dat'].max() - history_2weekago_df['article_total_latest_buy_2weekago']).dt.days\n",
    "\n",
    "    df = pd.merge(df,history_2weekago_df,how='left',on='article_id')\n",
    "\n",
    "    del history_2weekago, history_2weekago_df\n",
    "    gc.collect()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@noglobal\n",
    "def make_customer_features(customers):\n",
    "    return customers\n",
    "\n",
    "@noglobal\n",
    "def make_customer_tran_features(history, index_group_name_map):\n",
    "    group = ['Ladieswear', 'Divided', 'Menswear', 'Sport', 'Baby/Children']\n",
    "    for g in group:\n",
    "        history[g] = 0\n",
    "        g_id = index_group_name_map[g]\n",
    "        history.loc[history['index_group_name']==g_id, g] = 1\n",
    "\n",
    "\n",
    "    df = history.groupby('customer_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'],\n",
    "                                            'Ladieswear':'sum',\n",
    "                                            'Divided':'sum',\n",
    "                                            'Menswear':'sum',\n",
    "                                            'Sport':'sum',\n",
    "                                            'Baby/Children':'sum'}).reset_index()\n",
    "    df.columns = ['customer_id','customer_total_cnt', 'customer_total_latest_buy', 'customer_total_1st_buy', \n",
    "                  'customer_price_max', 'customer_price_min', 'customer_price_mean',\n",
    "                  'Ladieswear', 'Divided', 'Menswear', 'Sport', 'Baby/Children']\n",
    "    df['customer_total_1st_buy'] = (history['t_dat'].max() - df['customer_total_1st_buy']).dt.days\n",
    "    df['customer_total_latest_buy'] = (history['t_dat'].max() - df['customer_total_latest_buy']).dt.days\n",
    "\n",
    "    for g in group:\n",
    "        df[g] = df[g] / df['customer_total_cnt']\n",
    "\n",
    "#####################iida_exp24#########################\n",
    "    history_1weekago = history.loc[(history['t_dat'] > history['t_dat'].max() - relativedelta(days=7)) & \n",
    "                                   (history['t_dat'] <= history['t_dat'].max())]\n",
    "    history_1weekago_df =  history_1weekago.groupby('customer_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'],\n",
    "                                            'Ladieswear':'sum',\n",
    "                                            'Divided':'sum',\n",
    "                                            'Menswear':'sum',\n",
    "                                            'Sport':'sum',\n",
    "                                            'Baby/Children':'sum'}).reset_index() \n",
    "    history_1weekago_df.columns = ['customer_id','customer_total_cnt_1weekago', 'customer_total_latest_buy_1weekago', 'customer_total_1st_buy_1weekago', \n",
    "                                'customer_price_max_1weekago', 'customer_price_min_1weekago', 'customer_price_mean_1weekago',\n",
    "                                'Ladieswear_1weekago', 'Divided_1weekago', 'Menswear_1weekago', 'Sport_1weekago', 'Baby/Children_1weekago']\n",
    "\n",
    "    history_1weekago_df['customer_total_1st_buy_1weekago'] = (history_1weekago['t_dat'].max() - history_1weekago_df['customer_total_1st_buy_1weekago']).dt.days\n",
    "    history_1weekago_df['customer_total_latest_buy_1weekago'] = (history_1weekago['t_dat'].max() - history_1weekago_df['customer_total_latest_buy_1weekago']).dt.days\n",
    "\n",
    "    for g in group:\n",
    "        history_1weekago_df[g+'_1weekago'] = history_1weekago_df[g+'_1weekago'] / history_1weekago_df['customer_total_cnt_1weekago']\n",
    "\n",
    "    df = pd.merge(df,history_1weekago_df,how='left',on='customer_id')\n",
    "\n",
    "    history_2weekago = history.loc[(history['t_dat'] > history['t_dat'].max() - relativedelta(days=14)) & \n",
    "                                   (history['t_dat'] <= history['t_dat'].max() - relativedelta(days=7))]\n",
    "    history_2weekago_df =  history_2weekago.groupby('customer_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'],\n",
    "                                            'Ladieswear':'sum',\n",
    "                                            'Divided':'sum',\n",
    "                                            'Menswear':'sum',\n",
    "                                            'Sport':'sum',\n",
    "                                            'Baby/Children':'sum'}).reset_index() \n",
    "    history_2weekago_df.columns = ['customer_id','customer_total_cnt_2weekago', 'customer_total_latest_buy_2weekago', 'customer_total_1st_buy_2weekago', \n",
    "                                'customer_price_max_2weekago', 'customer_price_min_2weekago', 'customer_price_mean_2weekago',\n",
    "                                'Ladieswear_2weekago', 'Divided_2weekago', 'Menswear_2weekago', 'Sport_2weekago', 'Baby/Children_2weekago']\n",
    "\n",
    "    history_2weekago_df['customer_total_1st_buy_2weekago'] = (history_2weekago['t_dat'].max() - history_2weekago_df['customer_total_1st_buy_2weekago']).dt.days\n",
    "    history_2weekago_df['customer_total_latest_buy_2weekago'] = (history_2weekago['t_dat'].max() - history_2weekago_df['customer_total_latest_buy_2weekago']).dt.days\n",
    "\n",
    "    for g in group:\n",
    "        history_2weekago_df[g+'_2weekago'] = history_2weekago_df[g+'_2weekago'] / history_2weekago_df['customer_total_cnt_2weekago']\n",
    "\n",
    "    df = pd.merge(df,history_2weekago_df,how='left',on='customer_id')\n",
    "\n",
    "    del history_2weekago, history_2weekago_df\n",
    "    gc.collect()\n",
    "\n",
    "    return df\n",
    "\n",
    "@noglobal\n",
    "def make_customer_article_features(target, history):\n",
    "    df = target.merge(history, on=['customer_id', 'article_id'], how='inner')\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'t_dat':['count', 'min', 'max']}).reset_index()\n",
    "    df.columns = ['customer_id', 'article_id', 'count', '1st_buy_date_diff', 'latest_buy_date_diff']\n",
    "    df['1st_buy_date_diff'] = (history['t_dat'].max() - df['1st_buy_date_diff']).dt.days\n",
    "    df['latest_buy_date_diff'] = (history['t_dat'].max() - df['latest_buy_date_diff']).dt.days\n",
    "    return df\n",
    "\n",
    "@noglobal\n",
    "def add_same_article_type_rate(target, history, col):\n",
    "    add_data = history[['customer_id', col]].copy()\n",
    "    add_data['total'] = add_data.groupby('customer_id').transform('count')\n",
    "    add_data = add_data.groupby(['customer_id', col])['total'].agg(['max', 'count']).reset_index()\n",
    "    add_data[f'{col}_customer_buy_rate'] = add_data['count'] / add_data['max']\n",
    "    target = target.merge(add_data[['customer_id', col, f'{col}_customer_buy_rate']], on=['customer_id', col], how='left')\n",
    "    return target\n",
    "\n",
    "@noglobal\n",
    "def make_new_article_features(first_week_sales_pred):\n",
    "    first_week_sales_pred['new_article'] = 1\n",
    "    return first_week_sales_pred[['article_id', 'new_article', '1st_week_sales_pred']]\n",
    "    \n",
    "\n",
    "@noglobal\n",
    "def add_features(df, history, articles, customers, first_week_sales_pred, text_svd_df, index_group_name_map):\n",
    "    df = df.merge(make_article_features(articles), on=['article_id'], how='left')\n",
    "    df = df.merge(make_article_tran_features(history), on=['article_id'], how='left')\n",
    "    df = df.merge(make_customer_features(customers), on=['customer_id'], how='left')\n",
    "    df = df.merge(make_customer_tran_features(history, index_group_name_map), on=['customer_id'], how='left')\n",
    "    df = df.merge(make_customer_article_features(df[['customer_id', 'article_id']], history), on=['article_id', 'customer_id'], how='left')\n",
    "    df = df.merge(make_new_article_features(first_week_sales_pred), on=['article_id'], how='left')\n",
    "    df = df.merge(text_svd_df, on=['article_id'], how='left')\n",
    "\n",
    "    cols = ['product_type_name', 'product_group_name', 'graphical_appearance_name',\n",
    "            'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    "            'index_name', 'index_group_name', 'section_name', 'garment_group_name']\n",
    "\n",
    "    for c in cols:\n",
    "        df = add_same_article_type_rate(df, history, c)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レコメンド商品を購入するかどうかの2値分類モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(y_true, y_pred, K=12):\n",
    "    assert(len(y_true) == len(y_pred))\n",
    "    apks = []\n",
    "    for idx in range(len(y_true)):\n",
    "        y_i_true = y_true[idx]\n",
    "        y_i_pred = y_pred[idx]\n",
    "\n",
    "        # 予測値の数と重複の確認\n",
    "        assert(len(y_i_pred) <= K)\n",
    "        assert(len(np.unique(y_i_pred)) == len(y_i_pred))\n",
    "\n",
    "        sum_precision = 0.0\n",
    "        num_hits = 0.0\n",
    "\n",
    "        for i, p in enumerate(y_i_pred):\n",
    "            if p in y_i_true:\n",
    "                num_hits += 1\n",
    "                precision = num_hits / (i+1)\n",
    "                sum_precision += precision\n",
    "        apk = sum_precision / min(len(y_i_true), K)\n",
    "        apks.append(apk)\n",
    "    return apks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/cudf/core/series.py:923: FutureWarning: Series.set_index is deprecated and will be removed in the future\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_13055/3478087079.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_week_sales_pred['new_article'] = 1\n"
     ]
    }
   ],
   "source": [
    "if RUN_INF:\n",
    "    # テストデータの作成\n",
    "    all_target_id = sample['customer_id'].tolist()\n",
    "    first_week_sales_pred_tmp = first_week_sales_pred[first_week_sales_pred['1st_week_sales_dat'] >= '2020/09/23']\n",
    "\n",
    "    # メモリのケアのためバッチで推論を回す\n",
    "    batchs = [all_target_id[i:i+BATCH_SIZE] for i in range(0, len(all_target_id), BATCH_SIZE)]\n",
    "    target = 'buy'\n",
    "    not_use_cols = ['customer_id', 'article_id', target]\n",
    "    preds = []\n",
    "    \n",
    "    for target_id in batchs:\n",
    "        recom = get_reccomend(target_id, transactions, Ns, first_week_sales_pred_tmp)\n",
    "        ml_test = add_features(recom, transactions, articles, customers, first_week_sales_pred_tmp, text_svd_df, index_group_name_map)\n",
    "        features = [c for c in ml_test.columns if c not in not_use_cols]\n",
    "\n",
    "        test_pred = np.zeros(len(ml_test))\n",
    "        models = pathlib.Path(OUTPUT_DIR + f'{exp_name}').glob('model*.pickle')\n",
    "\n",
    "        for m in models:\n",
    "            with open(m, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "        test_pred += model.predict(ml_test[features], num_iteration=model.best_iteration) / N_SEED\n",
    "\n",
    "        test = ml_test[['customer_id', 'article_id']].copy()\n",
    "        test['prob'] = test_pred\n",
    "        test = test.sort_values(['customer_id', 'prob'], ascending=False)\n",
    "        test = test.groupby('customer_id').head(12)\n",
    "        preds.append(test)\n",
    "    \n",
    "    del recom, ml_test, test_pred\n",
    "    gc.collect()\n",
    "\n",
    "    test = pd.concat(preds)\n",
    "    test['article_id'] = test['article_id'].map(article_ids)\n",
    "    test['customer_id'] = test['customer_id'].map(customer_ids)\n",
    "\n",
    "    test = test.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "\n",
    "    sub = sample['customer_id'].map(customer_ids).to_frame()\n",
    "    sub = sub.merge(test, on=['customer_id'], how='left')\n",
    "    sub = sub.rename(columns={'article_id':'prediction'})\n",
    "    assert(sub['prediction'].apply(len).min()==12)\n",
    "    sub['prediction'] = sub['prediction'].apply(lambda x: ' '.join(x))\n",
    "    sub.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_sub.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
