{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import itertools\n",
    "import pickle\n",
    "import pathlib\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append(os.getenv('UTILS_PATH'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "import line_notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import types\n",
    "\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        # module imports\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield name, val\n",
    "\n",
    "            # functions / callables\n",
    "        if hasattr(val, '__call__'):\n",
    "            yield name, val\n",
    "\n",
    "\n",
    "def noglobal(f):\n",
    "    '''\n",
    "    ref: https://gist.github.com/raven38/4e4c3c7a179283c441f575d6e375510c\n",
    "    '''\n",
    "    return types.FunctionType(f.__code__,\n",
    "                              dict(imports()),\n",
    "                              f.__name__,\n",
    "                              f.__defaults__,\n",
    "                              f.__closure__\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_ITER = 1\n",
    "RUN_INF = False # 推論処理を行うか\n",
    "BATCH_SIZE = int(5e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = {}\n",
    "Ns['cf_a'] = 12\n",
    "Ns['ctf_a'] = 12\n",
    "Ns['atfd_a'] = 12\n",
    "Ns['atfp_a'] = 12\n",
    "Ns['pa_a'] = 12\n",
    "\n",
    "Ns['cf_w'] = 12\n",
    "Ns['ctf_w'] = 12\n",
    "Ns['atfd_w'] = 12\n",
    "Ns['atfp_w'] = 12\n",
    "Ns['pa_w'] = 12\n",
    "\n",
    "Ns['cf_m'] = 12\n",
    "Ns['ctf_m'] = 12\n",
    "Ns['atfd_m'] = 12\n",
    "Ns['atfp_m'] = 12\n",
    "Ns['pa_m'] = 12\n",
    "\n",
    "Ns['cf_y'] = 12\n",
    "Ns['ctf_y'] = 12\n",
    "Ns['atfd_y'] = 12\n",
    "Ns['atfp_y'] = 12\n",
    "Ns['pa_y'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディレクトリ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "#exp_name = os.path.dirname(__file__).split('/')[-1]\n",
    "#exp_name = 'exp034'\n",
    "#os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(INPUT_DIR + 'articles.csv', dtype='object')\n",
    "customers = pd.read_csv(INPUT_DIR + 'customers.csv')\n",
    "transactions = pd.read_csv(INPUT_DIR + 'transactions_train.csv', dtype={'article_id':'str'}, parse_dates=['t_dat'])\n",
    "sample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CUSTOMER = customers['customer_id'].unique().tolist()\n",
    "ALL_ARTICLE = articles['article_id'].unique().tolist()\n",
    "\n",
    "customer_ids = dict(list(enumerate(ALL_CUSTOMER)))\n",
    "article_ids = dict(list(enumerate(ALL_ARTICLE)))\n",
    "\n",
    "customer_map = {u: uidx for uidx, u in customer_ids.items()}\n",
    "article_map = {i: iidx for iidx, i in article_ids.items()}\n",
    "\n",
    "articles['article_id'] = articles['article_id'].map(article_map)\n",
    "customers['customer_id'] = customers['customer_id'].map(customer_map)\n",
    "transactions['article_id'] = transactions['article_id'].map(article_map)\n",
    "transactions['customer_id'] = transactions['customer_id'].map(customer_map)\n",
    "sample['customer_id'] = sample['customer_id'].map(customer_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名寄せ\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].str.replace('None','NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['age10'] = str((customers['age'] // 10) * 10)\n",
    "customers.loc[customers['age'].isnull(), 'age10'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoding\n",
    "le_cols = ['product_type_name', 'product_group_name', 'graphical_appearance_name',\n",
    "            'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    "            'index_name', 'index_group_name', 'section_name', 'garment_group_name']\n",
    "for c in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    articles[c] = le.fit_transform(articles[c].fillna(''))\n",
    "\n",
    "\n",
    "le_cols = ['club_member_status', 'fashion_news_frequency', 'postal_code', 'age10']\n",
    "for c in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    customers[c] = le.fit_transform(customers[c].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['customer_type'] = customers['FN'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['Active'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['club_member_status'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['fashion_news_frequency'].fillna(0).astype(int).astype(str) + \\\n",
    "                             customers['age10'].fillna(0).astype(int).astype(str)\n",
    "\n",
    "le = LabelEncoder()\n",
    "customers['customer_type'] = le.fit_transform(customers['customer_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactionに紐づけ\n",
    "transactions = transactions.merge(customers, on='customer_id', how='left')\n",
    "transactions = transactions.merge(articles, on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセット作成（レコメンド→対象データセット作成→特徴量エンジニアリング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def get_customer_frequent(history, n=12, timedelta=None):\n",
    "    \"\"\"顧客ごと商品の購入数をカウントし上位の商品を抽出\n",
    "\n",
    "    Args:\n",
    "        history (dataframe): 集計対象の実績データ\n",
    "        n (int): レコメンド対象とする数\n",
    "        timedelta (dateutil.relativedelta): 指定された場合、実績データの終端からtimedelta分のデータを取得する\n",
    "\n",
    "    Returns:\n",
    "        dataframe: 抽出結果\n",
    "    \"\"\"\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "        \n",
    "    customer_agg = history.groupby(['customer_id', 'article_id'])['t_dat'].count().reset_index()\n",
    "    customer_agg = customer_agg.rename(columns={'t_dat':'cnt'})\n",
    "    customer_agg = customer_agg.sort_values(['customer_id', 'cnt'], ascending=False)\n",
    "    result = customer_agg.groupby('customer_id').head(n)\n",
    "    return result[['customer_id', 'article_id']]\n",
    "\n",
    "@noglobal\n",
    "def get_popular_article(history, n=12, timedelta=None):\n",
    "    \"\"\"全体の購入数をカウントし上位の商品を抽出\n",
    "\n",
    "    Args:\n",
    "        history (dataframe): 集計対象の実績データ\n",
    "        n (int): レコメンド対象とする数\n",
    "        timedelta (dateutil.relativedelta): 指定された場合、実績データの終端からtimedelta分のデータを取得する\n",
    "\n",
    "    Returns:\n",
    "        list: 抽出結果\n",
    "    \"\"\"\n",
    "    # 全体の購入数量\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "\n",
    "    total_agg = history.groupby('article_id')['t_dat'].count().reset_index()\n",
    "    total_agg = total_agg.rename(columns={'t_dat':'cnt'})\n",
    "    total_agg = total_agg.sort_values(['cnt'], ascending=False)\n",
    "    total_agg = total_agg.head(n)\n",
    "    result = list(total_agg['article_id'].values)\n",
    "    return result\n",
    "\n",
    "@noglobal\n",
    "def get_customer_type_frequent(history, n=12, timedelta=None):\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "\n",
    "    result = history[['customer_id', 'customer_type']].drop_duplicates().copy()\n",
    "    agg = history.groupby(['customer_type', 'article_id'])['t_dat'].count().reset_index()\n",
    "    agg = agg.rename(columns={'t_dat':'cnt'})\n",
    "    agg = agg.sort_values(['customer_type', 'cnt'], ascending=False)\n",
    "    agg = agg.groupby('customer_type').head(n)\n",
    "    result = result.merge(agg[['customer_type', 'article_id']], on='customer_type', how='left')\n",
    "    return result[['customer_id', 'article_id']]\n",
    "\n",
    "@noglobal\n",
    "def get_article_type_frequent(history, col, n=12, timedelta=None):\n",
    "    if timedelta is not None:\n",
    "        st_date = history['t_dat'].max() - timedelta\n",
    "        history = history[history['t_dat']>=st_date].copy()\n",
    "\n",
    "    result = history.groupby(['customer_id', col])['t_dat'].count().reset_index()\n",
    "    result = result.rename(columns={'t_dat':'cnt'})\n",
    "    result = result.sort_values(['customer_id', 'cnt'], ascending=False)\n",
    "    result = result.groupby(['customer_id']).head(1)[['customer_id', col]]\n",
    "\n",
    "    agg = history.groupby([col, 'article_id'])['t_dat'].count().reset_index()\n",
    "    agg = agg.rename(columns={'t_dat':'cnt'})\n",
    "    agg = agg.sort_values([col, 'cnt'], ascending=False)\n",
    "    agg = agg.groupby(col).head(n)\n",
    "    result = result.merge(agg[[col, 'article_id']], on=col, how='left')\n",
    "    return result[['customer_id', 'article_id']]\n",
    "\n",
    "@noglobal\n",
    "def get_reccomend(target_customer_id, history, Ns):\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    td = None\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_a'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_a'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_a'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_a'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_a'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    td = relativedelta(weeks=1)\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_w'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_w'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_w'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_w'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_w'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    td = relativedelta(months=1)\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_m'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_m'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_m'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_m'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_m'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    td = relativedelta(years=1)\n",
    "    result = result.append(get_customer_frequent(history, Ns['cf_y'], td))\n",
    "    result = result.append(get_customer_type_frequent(history, Ns['ctf_y'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'department_name', Ns['atfd_y'], td))\n",
    "    result = result.append(get_article_type_frequent(history, 'perceived_colour_master_name', Ns['atfp_y'], td))\n",
    "    popular_article = get_popular_article(history, Ns['pa_y'], td)\n",
    "    # customerとpopular articleの全組み合わせでdataframe作成\n",
    "    popular_article = pd.DataFrame(itertools.product(target_customer_id, popular_article), columns=['customer_id', 'article_id'])\n",
    "    result = result.append(popular_article)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    result = result[result['customer_id'].isin(target_customer_id)].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def add_labels(recom_result, history):\n",
    "    \"\"\"レコメンドしたデータが学習期間で購入されたかどうかのフラグを付与する\n",
    "\n",
    "    Args:\n",
    "        recom_result (_type_): レコメンド結果\n",
    "        train_tran (_type_): 学習期間のトランザクションデータ\n",
    "\n",
    "    Returns:\n",
    "        _type_: 学習期間での購入フラグを付与したレコメンド結果\n",
    "    \"\"\"\n",
    "    history = history[['customer_id', 'article_id']].drop_duplicates()\n",
    "    history['buy'] = 1\n",
    "    recom_result = recom_result.merge(history, on=['customer_id', 'article_id'], how='left')\n",
    "    recom_result['buy'] = recom_result['buy'].fillna(0)\n",
    "    return recom_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def make_article_features(articles):\n",
    "    cols = ['product_type_name', 'product_group_name', 'graphical_appearance_name',\n",
    "            'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    "            'index_name', 'index_group_name', 'section_name', 'garment_group_name']\n",
    "    return articles[['article_id']+cols]\n",
    "\n",
    "@noglobal\n",
    "def make_article_tran_features(history):\n",
    "    df = history.groupby('article_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean'], \n",
    "                                            'age':['max', 'min', 'mean', 'std']}).reset_index()\n",
    "    df.columns = ['article_id','article_total_cnt', 'article_total_latest_buy', 'article_total_1st_buy', 'article_price_max', 'article_price_min', 'article_price_mean', 'article_age_max', 'article_age_min', 'article_age_mean', 'article_age_std']\n",
    "    df['article_total_1st_buy'] = (history['t_dat'].max() - df['article_total_1st_buy']).dt.days\n",
    "    df['article_total_latest_buy'] = (history['t_dat'].max() - df['article_total_latest_buy']).dt.days\n",
    "    return df\n",
    "\n",
    "\n",
    "@noglobal\n",
    "def make_customer_features(customers):\n",
    "    return customers\n",
    "\n",
    "@noglobal\n",
    "def make_customer_tran_features(history):\n",
    "    df = history.groupby('customer_id').agg({'t_dat':['count', 'max', 'min'],\n",
    "                                            'price':['max', 'min', 'mean']}).reset_index()\n",
    "    df.columns = ['customer_id','customer_total_cnt', 'customer_total_latest_buy', 'customer_total_1st_buy', 'customer_price_max', 'customer_price_min', 'customer_price_mean']\n",
    "    df['customer_total_1st_buy'] = (history['t_dat'].max() - df['customer_total_1st_buy']).dt.days\n",
    "    df['customer_total_latest_buy'] = (history['t_dat'].max() - df['customer_total_latest_buy']).dt.days\n",
    "    return df\n",
    "\n",
    "@noglobal\n",
    "def make_customer_article_features(target, history):\n",
    "    df = target.merge(history, on=['customer_id', 'article_id'], how='inner')\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'t_dat':['count', 'min', 'max']}).reset_index()\n",
    "    df.columns = ['customer_id', 'article_id', 'count', '1st_buy_date_diff', 'latest_buy_date_diff']\n",
    "    df['1st_buy_date_diff'] = (history['t_dat'].max() - df['1st_buy_date_diff']).dt.days\n",
    "    df['latest_buy_date_diff'] = (history['t_dat'].max() - df['latest_buy_date_diff']).dt.days\n",
    "    return df\n",
    "\n",
    "@noglobal\n",
    "def add_same_article_type_rate(target, history, col):\n",
    "    add_data = history[['customer_id', col]].copy()\n",
    "    add_data['total'] = add_data.groupby('customer_id').transform('count')\n",
    "    add_data = add_data.groupby(['customer_id', col])['total'].agg(['max', 'count']).reset_index()\n",
    "    add_data[f'{col}_customer_buy_rate'] = add_data['count'] / add_data['max']\n",
    "    target = target.merge(add_data[['customer_id', col, f'{col}_customer_buy_rate']], on=['customer_id', col], how='left')\n",
    "    return target\n",
    "\n",
    "    \n",
    "\n",
    "@noglobal\n",
    "def add_features(df, history, articles, customers):\n",
    "    df = df.merge(make_article_features(articles), on=['article_id'], how='left')\n",
    "    df = df.merge(make_article_tran_features(history), on=['article_id'], how='left')\n",
    "    df = df.merge(make_customer_features(customers), on=['customer_id'], how='left')\n",
    "    df = df.merge(make_customer_tran_features(history), on=['customer_id'], how='left')\n",
    "    df = df.merge(make_customer_article_features(df[['customer_id', 'article_id']], history), on=['article_id', 'customer_id'], how='left')\n",
    "\n",
    "    cols = ['product_type_name', 'product_group_name', 'graphical_appearance_name',\n",
    "            'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    "            'index_name', 'index_group_name', 'section_name', 'garment_group_name']\n",
    "\n",
    "    for c in cols:\n",
    "        df = add_same_article_type_rate(df, history, c)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レコメンド商品を購入するかどうかの2値分類モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(y_true, y_pred, K=12):\n",
    "    assert(len(y_true) == len(y_pred))\n",
    "    apks = []\n",
    "    for idx in range(len(y_true)):\n",
    "        y_i_true = y_true[idx]\n",
    "        y_i_pred = y_pred[idx]\n",
    "\n",
    "        # 予測値の数と重複の確認\n",
    "        assert(len(y_i_pred) <= K)\n",
    "        assert(len(np.unique(y_i_pred)) == len(y_i_pred))\n",
    "\n",
    "        sum_precision = 0.0\n",
    "        num_hits = 0.0\n",
    "\n",
    "        for i, p in enumerate(y_i_pred):\n",
    "            if p in y_i_true:\n",
    "                num_hits += 1\n",
    "                precision = num_hits / (i+1)\n",
    "                sum_precision += precision\n",
    "        apk = sum_precision / min(len(y_i_true), K)\n",
    "        apks.append(apk)\n",
    "    return apks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def run_train(transactions, articles, customers, Ns):\n",
    "    # 1週ずつローリングして学習データを生成し検証\n",
    "    train_start = datetime.datetime(2020,9,9)\n",
    "    valid_start = datetime.datetime(2020,9,16)\n",
    "    valid_end = datetime.datetime(2020,9,22)\n",
    "\n",
    "    # 学習データの作成\n",
    "    history_tran = transactions[transactions['t_dat'] < train_start].copy()\n",
    "    target_tran = transactions[(transactions['t_dat'] >= train_start) & (transactions['t_dat'] < valid_start)].copy()\n",
    "    target_id = target_tran['customer_id'].unique().tolist()\n",
    "    recom = get_reccomend(target_id, history_tran, Ns)\n",
    "    ml_train = add_labels(recom, target_tran)\n",
    "    ml_train = add_features(ml_train, history_tran, articles, customers)\n",
    "\n",
    "    # 評価データの作成\n",
    "    history_tran = transactions[transactions['t_dat'] < valid_start].copy()\n",
    "    target_tran = transactions[(transactions['t_dat'] >= valid_start) & (transactions['t_dat'] <= valid_end)].copy()\n",
    "    target_id = target_tran['customer_id'].unique().tolist()\n",
    "    recom = get_reccomend(target_id, history_tran, Ns)\n",
    "    ml_valid = add_labels(recom, target_tran)\n",
    "    ml_valid = add_features(ml_valid, history_tran, articles, customers)\n",
    "\n",
    "    target = 'buy'\n",
    "    not_use_cols = ['customer_id', 'article_id', target]\n",
    "    features = [c for c in ml_train.columns if c not in not_use_cols]\n",
    "\n",
    "    params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\" : \"gbdt\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "    # 学習\n",
    "    tr_x, tr_y = ml_train[features], ml_train[target]\n",
    "    vl_x, vl_y = ml_valid[features], ml_valid[target]\n",
    "    tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "    model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                    num_boost_round=20000, early_stopping_rounds=100,verbose_eval=1000)\n",
    "\n",
    "    # cv\n",
    "    vl_pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "    # 正解データ作成\n",
    "    valid = transactions[(transactions['t_dat'] >= valid_start) & (transactions['t_dat'] <= valid_end)].copy()\n",
    "    valid = valid[['customer_id', 'article_id']].drop_duplicates()\n",
    "    valid = valid.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "    valid = valid.sort_values('customer_id').reset_index(drop=True)\n",
    "    # 2値分類の出力を元に12個選定\n",
    "    valid_pred = ml_valid[['customer_id', 'article_id']].copy()\n",
    "    valid_pred['prob'] = vl_pred\n",
    "    valid_pred = valid_pred.sort_values(['customer_id', 'prob'], ascending=False)\n",
    "    valid_pred = valid_pred.groupby('customer_id').head(12)\n",
    "    valid_pred = valid_pred.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "    valid_pred = valid_pred.sort_values('customer_id').reset_index(drop=True)\n",
    "    assert(valid['customer_id'].tolist() == valid_pred['customer_id'].tolist())\n",
    "    # MAP@12\n",
    "    score = np.mean(apk(valid['article_id'].tolist(), valid_pred['article_id'].tolist()))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    cf = trial.suggest_int('cf', 0, 5)\n",
    "    ctf = trial.suggest_int('ctf', 0, 5)\n",
    "    atfd = trial.suggest_int('atfd', 0, 5)\n",
    "    atfp = trial.suggest_int('atfp', 0, 5)\n",
    "    pa = trial.suggest_int('pa', 0, 5)\n",
    "\n",
    "    a = trial.suggest_int('a', 0, 5)\n",
    "    w = trial.suggest_int('w', 0, 5)\n",
    "    m = trial.suggest_int('m', 0, 5)\n",
    "    y = trial.suggest_int('y', 0, 5)\n",
    "    \n",
    "    Ns['cf_a'] = cf * a\n",
    "    Ns['ctf_a'] = ctf * a\n",
    "    Ns['atfd_a'] = atfd * a\n",
    "    Ns['atfp_a'] = atfp * a\n",
    "    Ns['pa_a'] = pa * a\n",
    "\n",
    "    Ns['cf_w'] = cf * w\n",
    "    Ns['ctf_w'] = ctf * w\n",
    "    Ns['atfd_w'] = atfd * w\n",
    "    Ns['atfp_w'] = atfp * w\n",
    "    Ns['pa_w'] = pa * w\n",
    "\n",
    "    Ns['cf_m'] = cf * m\n",
    "    Ns['ctf_m'] = ctf * m\n",
    "    Ns['atfd_m'] = atfd * m\n",
    "    Ns['atfp_m'] = atfp * m\n",
    "    Ns['pa_m'] = pa * m\n",
    "\n",
    "    Ns['cf_y'] = cf * y\n",
    "    Ns['ctf_y'] = ctf * y\n",
    "    Ns['atfd_y'] = atfd * y\n",
    "    Ns['atfp_y'] = atfp * y\n",
    "    Ns['pa_y'] = pa * y\n",
    "\n",
    "    total_n = Ns['ctf_a'] + Ns['pa_a'] + \\\n",
    "              Ns['ctf_w'] + Ns['pa_w'] + \\\n",
    "              Ns['ctf_m'] + Ns['pa_m'] + \\\n",
    "              Ns['ctf_y'] + Ns['pa_y']\n",
    "\n",
    "    if total_n > 12:\n",
    "        score = run_train(transactions, articles, customers, Ns)\n",
    "    else:\n",
    "        score = 0.0\n",
    "    message = f'{Ns}\\n{score}'\n",
    "    line_notify.send(message)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-06 12:25:00,955]\u001b[0m A new study created in memory with name: no-name-dc5ac115-bb8d-4b5e-967f-260751fd6fb0\u001b[0m\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12734, number of negative: 5101614\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.639817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7545\n",
      "[LightGBM] [Info] Number of data points in the train set: 5114348, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002490 -> initscore=-5.993037\n",
      "[LightGBM] [Info] Start training from score -5.993037\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.0138502\tvalid_1's binary_logloss: 0.014922\n",
      "[2000]\ttraining's binary_logloss: 0.0127597\tvalid_1's binary_logloss: 0.0149065\n",
      "Early stopping, best iteration is:\n",
      "[2424]\ttraining's binary_logloss: 0.0123653\tvalid_1's binary_logloss: 0.0149039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-06 12:43:41,796]\u001b[0m Trial 0 finished with value: 0.017594331251912475 and parameters: {'cf': 0, 'ctf': 0, 'atfd': 5, 'atfp': 3, 'pa': 4, 'a': 3, 'w': 0, 'm': 4, 'y': 4}. Best is trial 0 with value: 0.017594331251912475.\u001b[0m\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8819, number of negative: 2463641\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7695\n",
      "[LightGBM] [Info] Number of data points in the train set: 2472460, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003567 -> initscore=-5.632487\n",
      "[LightGBM] [Info] Start training from score -5.632487\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.0163767\tvalid_1's binary_logloss: 0.0186064\n",
      "Early stopping, best iteration is:\n",
      "[916]\ttraining's binary_logloss: 0.0165357\tvalid_1's binary_logloss: 0.0186017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-06 12:55:11,510]\u001b[0m Trial 1 finished with value: 0.026153086501631787 and parameters: {'cf': 2, 'ctf': 1, 'atfd': 0, 'atfp': 3, 'pa': 2, 'a': 5, 'w': 0, 'm': 2, 'y': 1}. Best is trial 1 with value: 0.026153086501631787.\u001b[0m\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 14374, number of negative: 4948045\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.543172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7693\n",
      "[LightGBM] [Info] Number of data points in the train set: 4962419, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002897 -> initscore=-5.841327\n",
      "[LightGBM] [Info] Start training from score -5.841327\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.0145848\tvalid_1's binary_logloss: 0.0159216\n",
      "[2000]\ttraining's binary_logloss: 0.0135165\tvalid_1's binary_logloss: 0.0158983\n",
      "Early stopping, best iteration is:\n",
      "[2123]\ttraining's binary_logloss: 0.0133956\tvalid_1's binary_logloss: 0.0158973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-03-06 13:15:30,609]\u001b[0m Trial 2 failed because of the following error: AssertionError()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaggler/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_20560/1751778146.py\", line 43, in objective\n",
      "    score = run_train(transactions, articles, customers, Ns)\n",
      "  File \"/tmp/ipykernel_20560/1822577249.py\", line 58, in run_train\n",
      "    assert(valid['customer_id'].tolist() == valid_pred['customer_id'].tolist())\n",
      "AssertionError\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20560/3653606818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20560/1751778146.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20560/1822577249.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(transactions, articles, customers, Ns)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mvalid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mvalid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalid_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customer_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# MAP@12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cf_a': 12, 'ctf_a': 9, 'atfd_a': 8, 'atfp_a': 12, 'pa_a': 2, 'cf_w': 8, 'ctf_w': 14, 'atfd_w': 23, 'atfp_w': 12, 'pa_w': 12, 'cf_m': 15, 'ctf_m': 10, 'atfd_m': 15, 'atfp_m': 11, 'pa_m': 5, 'cf_y': 19, 'ctf_y': 22, 'atfd_y': 3, 'atfp_y': 11, 'pa_y': 13}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
